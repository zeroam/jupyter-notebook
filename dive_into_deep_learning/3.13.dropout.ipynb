{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "3.13.dropout.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zeroam/jupyter-notebook/blob/master/dive_into_deep_learning/3.13.dropout.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1yzK2qUxrOLI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install mxnet==1.6.0b20190915\n",
        "!pip install d2l"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r1fOPD2MshOu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import d2l\n",
        "from mxnet import autograd, gluon, init, nd\n",
        "from mxnet.gluon import loss as gloss, nn\n",
        "\n",
        "def dropout(X, drop_prob):\n",
        "  assert 0 <= drop_prob <= 1\n",
        "  # In this case, all elements are dropped out\n",
        "  if drop_prob == 1:\n",
        "    return X.zeros_like()\n",
        "  mask = nd.random.uniform(0, 1, X.shape) > drop_prob\n",
        "  return mask * X / (1.0-drop_prob)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sCTPwIsbtDaU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        },
        "outputId": "5ff7e569-8cd5-44c7-e06e-88e6cdbfd6b7"
      },
      "source": [
        "X = nd.arange(16).reshape((2, 8))\n",
        "print(dropout(X, 0))\n",
        "print(dropout(X, 0.5))\n",
        "print(dropout(X, 1))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "[[ 0.  1.  2.  3.  4.  5.  6.  7.]\n",
            " [ 8.  9. 10. 11. 12. 13. 14. 15.]]\n",
            "<NDArray 2x8 @cpu(0)>\n",
            "\n",
            "[[ 0.  0.  0.  0.  8. 10. 12.  0.]\n",
            " [16.  0. 20. 22.  0.  0.  0. 30.]]\n",
            "<NDArray 2x8 @cpu(0)>\n",
            "\n",
            "[[0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "<NDArray 2x8 @cpu(0)>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WERN8DS_tTvH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_inputs, num_outputs, num_hiddens1, num_hiddens2 = 784, 10, 256, 256\n",
        "\n",
        "W1 = nd.random.normal(scale=0.01, shape=(num_inputs, num_hiddens1))\n",
        "b1 = nd.zeros(num_hiddens1)\n",
        "W2 = nd.random.normal(scale=0.01, shape=(num_hiddens1, num_hiddens2))\n",
        "b2 = nd.zeros(num_hiddens2)\n",
        "W3 = nd.random.normal(scale=0.01, shape=(num_hiddens2, num_outputs))\n",
        "b3 = nd.zeros(num_outputs)\n",
        "\n",
        "params = [W1, b1, W2, b2, W3, b3]\n",
        "for param in params:\n",
        "  param.attach_grad()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FBHRaKtjt7rI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "drop_prob1, drop_prob2 = 0.2, 0.5\n",
        "\n",
        "def net(X):\n",
        "  X = X.reshape((-1, num_inputs))\n",
        "  H1 = (nd.dot(X, W1) + b1).relu()\n",
        "  # Use dropout only when training the model\n",
        "  if autograd.is_training():\n",
        "    # Add a dropout layer after the first fully connected layer\n",
        "    H1 = dropout(H1, drop_prob1)\n",
        "  H2 = (nd.dot(H1, W2) + b2).relu()\n",
        "  if autograd.is_training():\n",
        "    # Add a dropout layer after the second fully connected layer\n",
        "    H2 = dropout(H2, drop_prob2)\n",
        "  return nd.dot(H2, W3) + b3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F3LXrW_nu-KN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def evaluate_accuracy(data_iter, net):\n",
        "    acc_sum, n = 0.0, 0\n",
        "    for X, y in data_iter:\n",
        "        y = y.astype('float32')\n",
        "        acc_sum += (net(X).argmax(axis=1) == y).sum().asscalar()\n",
        "        n += y.size\n",
        "    return acc_sum / n\n",
        "\n",
        "def train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size,\n",
        "              params=None, lr=None, trainer=None):\n",
        "    for epoch in range(num_epochs):\n",
        "        train_l_sum, train_acc_sum, n = 0.0, 0.0, 0\n",
        "        for X, y in train_iter:\n",
        "            with autograd.record():\n",
        "                y_hat = net(X)\n",
        "                l = loss(y_hat, y).sum()\n",
        "            l.backward()\n",
        "            if trainer is None:\n",
        "                d2l.sgd(params, lr, batch_size)\n",
        "            else:\n",
        "                # This will be illustrated in the next section\n",
        "                trainer.step(batch_size)\n",
        "            y = y.astype('float32')\n",
        "            train_l_sum += l.asscalar()\n",
        "            train_acc_sum += (y_hat.argmax(axis=1) == y).sum().asscalar()\n",
        "            n += y.size\n",
        "        test_acc = evaluate_accuracy(test_iter, net)\n",
        "        print('epoch %d, loss %.4f, train acc %.3f, test acc %.3f'\n",
        "              % (epoch + 1, train_l_sum / n, train_acc_sum / n, test_acc))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1rj-ZWWUunf0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "c5ab48cb-8a5c-4533-e533-2c8a25b0680e"
      },
      "source": [
        "num_epochs, lr, batch_size = 10, 0.5, 256\n",
        "loss = gloss.SoftmaxCrossEntropyLoss()\n",
        "train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)\n",
        "train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size, params, lr)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading /root/.mxnet/datasets/fashion-mnist/train-images-idx3-ubyte.gz from https://apache-mxnet.s3-accelerate.dualstack.amazonaws.com/gluon/dataset/fashion-mnist/train-images-idx3-ubyte.gz...\n",
            "Downloading /root/.mxnet/datasets/fashion-mnist/train-labels-idx1-ubyte.gz from https://apache-mxnet.s3-accelerate.dualstack.amazonaws.com/gluon/dataset/fashion-mnist/train-labels-idx1-ubyte.gz...\n",
            "Downloading /root/.mxnet/datasets/fashion-mnist/t10k-images-idx3-ubyte.gz from https://apache-mxnet.s3-accelerate.dualstack.amazonaws.com/gluon/dataset/fashion-mnist/t10k-images-idx3-ubyte.gz...\n",
            "Downloading /root/.mxnet/datasets/fashion-mnist/t10k-labels-idx1-ubyte.gz from https://apache-mxnet.s3-accelerate.dualstack.amazonaws.com/gluon/dataset/fashion-mnist/t10k-labels-idx1-ubyte.gz...\n",
            "epoch 1, loss 1.1768, train acc 0.541, test acc 0.777\n",
            "epoch 2, loss 0.5997, train acc 0.779, test acc 0.835\n",
            "epoch 3, loss 0.5030, train acc 0.816, test acc 0.844\n",
            "epoch 4, loss 0.4543, train acc 0.834, test acc 0.862\n",
            "epoch 5, loss 0.4305, train acc 0.843, test acc 0.848\n",
            "epoch 6, loss 0.4020, train acc 0.854, test acc 0.867\n",
            "epoch 7, loss 0.3876, train acc 0.859, test acc 0.870\n",
            "epoch 8, loss 0.3733, train acc 0.865, test acc 0.863\n",
            "epoch 9, loss 0.3635, train acc 0.867, test acc 0.870\n",
            "epoch 10, loss 0.3554, train acc 0.870, test acc 0.872\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hmjrs4RxvEIp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "net = nn.Sequential()\n",
        "net.add(nn.Dense(256, activation='relu'),\n",
        "        # Add a dropout layer after the first fully connected layer\n",
        "        nn.Dropout(drop_prob1),\n",
        "        nn.Dense(256, activation='relu'),\n",
        "        # Add a dropout layer after the second fully connected layer\n",
        "        nn.Dropout(drop_prob2),\n",
        "        nn.Dense(10))\n",
        "net.initialize(init.Normal(sigma=0.01))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Jh6Adzbxrec",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        },
        "outputId": "4f59d142-becd-48ee-9231-8301ab84436f"
      },
      "source": [
        "trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': lr})\n",
        "train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size, None,\n",
        "          None, trainer)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch 1, loss 1.2351, train acc 0.526, test acc 0.786\n",
            "epoch 2, loss 0.5943, train acc 0.778, test acc 0.833\n",
            "epoch 3, loss 0.5116, train acc 0.814, test acc 0.833\n",
            "epoch 4, loss 0.4565, train acc 0.833, test acc 0.862\n",
            "epoch 5, loss 0.4280, train acc 0.842, test acc 0.864\n",
            "epoch 6, loss 0.4087, train acc 0.852, test acc 0.868\n",
            "epoch 7, loss 0.3959, train acc 0.855, test acc 0.868\n",
            "epoch 8, loss 0.3809, train acc 0.861, test acc 0.867\n",
            "epoch 9, loss 0.3648, train acc 0.867, test acc 0.876\n",
            "epoch 10, loss 0.3581, train acc 0.869, test acc 0.866\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GgrSMJnRx-O1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}